---
title: "Ecommerce"
author: "Daniel Ramos"
date: "24/1/2021"
output: 
  html_document:
    toc: true
    toc_depth: 5
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(formattable)
library(arules)
library(arulesViz)
library(ggplot2)
library(cluster)
library(gbm)
library(ISLR)
```

## Parte 1 Contexto del problema y modelo de datos (30/100)

### 1.Contextualización

El fichero del cual se extraen los datos es data.csv, el cual contiene la información de las transacciones de un comerciante al por menor de Reino Unido.
El fichero esta compuesto por 8 columnas y más de medio millón de filas.

Las columnas son las siguentes:

- Nº de Factura: **InvoiceNo**
- Código de Stock: **StockCode**
- Descripción: **Description**
- Cantidad: **Quantity**
- Fecha de Factura: **InvoiceDate**
- Precio por unidad: **UnitPrice**
- ID del comprador: **CustomerID**
- País: **Country**

Con los siguientes datos se van a realizar una serie de gráficos, tablas y análisis mediante técnicas de inferencia estadística y machine learning para obtener una visión global de esta empresa B2B para ayudar a la hora de la toma de decisiones.

### 2. Limpieza de datos

En esta sección limpiamos el fichero de datos eliminando valores NA, observaciones repetidas y valores erróneos. También creamos la variables nuevas como el precio total de cada venta, parseamos la fecha con la ayuda de el paquete Lubridate, convertimos y separamos los datos a un formato más cómodo para analizar.

```{r carga, results='hide'}
#Cargamos los datos:
data <- read.csv("data.csv")
class(data$InvoiceDate)
#A priori parace que solo tenemos problemas con la fecha de la factura que es leída como un string
data <- data %>%
  mutate(
    ParsedDate = parse_datetime(InvoiceDate, format = "%m/%d/%Y %H:%M")
  )
#Comprobamos que no haya NA en el fichero
anyNA(data) #Tenemos NA
summary(data) #Solo hay NA's en CustomerID

#Borramos todos los precios y cantidades negativas
data <- data %>% 
  mutate(Quantity = replace(Quantity, Quantity<=0, NA),
         UnitPrice = replace(UnitPrice, UnitPrice<=0, NA))
#Borramos los NA
data <- data %>%
  drop_na()

#Sustituimos los NA por 0, son 135080 observaciones que pueden contener información interesante
#data$CustomerID <- ifelse(is.na(data$CustomerID)== TRUE,0,data$CustomerID)
#Si hay duplicados los eliminamos con la función de dplyr
data <- data %>%
  distinct()

#Añadimos una columna con el precio total de cada venta:
data <- data %>% mutate(lineTotal = Quantity * UnitPrice)

#Añadimos una columna con el dia de la semana
data$weekDay <- wday(data$ParsedDate, label=TRUE)
#Como la etiquetas que genera automáticamente son horribles vamos a renombrarlas
levels(data$weekDay) <- c("Dom","Lun","Mar","Mie","Jue","Vie","Sab")

#Añadimos una columna con el mes del año
data$month <- month(data$ParsedDate, label = TRUE)

#Convertimos los paises en un factor
data$Country <- as.factor(data$Country)
```

### 3.Justificación del tipo de variables

- Nº de Factura: Es una variable categórica ordinal, ya que el orden sí que es importante. Se mantendrá esta variable como una cadena de caracteres por una sencilla razón, no todas las facturas son númericas, hay algunas que empiezan por la letra C, que significan una cancelación.

- Código de Stock: Es una variable categórica, ya que tiene un valor finito, pero la mantendremos como un string ya que tiene valores alfanúmericos.

- Descripción: Es una variable categórica única para cada producto y la manera más comoda de usarla es mediante caracteres ya que son muchos productos y convertirla en factor seria ineficiente.

- Cantidad: Es una variable númerica y la mantendremos como un entero.

- Fecha de Factura: Es una variable ordinal, ya que el orden es importante y tiene un rango de valores finito, aún asi se ha parseado la varible para que R puede interpretarla en formato fecha.

- Precio por unidad: Es una variable númerica continua.

- ID del comprador: Esta variable es categorica que representa a cada comprador, pero por comodidad para el análisis la dejaremos como variable númerica.

- País: Variable categórica que representa a cada país y la convertimos en un factor.


## Parte 2: Análisis exploratorio (EDA). (30/100)

#### Estadísticas generales sobre la cantidad de productos que vendemos:

```{r stats_generales}
data %>%
  group_by(StockCode) %>%
  summarise(count = n(),
            sum_quantity = sum(Quantity),
            mean_quantity = round(mean(Quantity), 2),
            median_quantity = median(Quantity),
            mode_quantity = unique(Quantity)[which.max(tabulate(match(Quantity, unique(Quantity))))],
            sd_quantity = round(sd(Quantity), 2))
```

#### 10 Productos más vendidos:

```{r top_10, warning=FALSE, message=FALSE}
#Detalle importante a tener en cuenta: El precio de los productos son menores si se compran en grandes cantidades:
data %>%
  group_by(StockCode, UnitPrice) %>%
  summarise(Total = sum(Quantity)) %>% head(10)

#Observamos la cantidad total vendida de cada producto
sales <- data %>%
  group_by(StockCode) %>%
  summarise(Total = sum(Quantity))

#Ordenamos de mayor a menor
sales <- sales[
  with(sales, order(-Total)),
]
sales %>% top_n(10) %>%
  formattable(align = c("l", "c"),
              list(Total = color_bar("lightgrey")),
              col.names = c("Código","Nº Vendido"))
  
```

#### Productos menos vendidos:

```{r least_sold}
#Tenemos muchos productos que se han vendido menos de 10 veces
sum(sales$Total < 10) #En concreto 338
#Quizás lo mejor sería eliminar estos productos del Stock
head(sales[sales$Total < 10,])
```
#### Datos totales:

```{r total_stats}
#Total de productos vendidos, total clientes, total dinero ganado, facturas y paises
#Sumamos las columnas de cantidades y beneficio
sums <- data %>% 
  select(Quantity,lineTotal) %>%
  summarise_all(sum)

#Cogemos los valores únicos
unic <- data %>% select(InvoiceNo, StockCode, CustomerID, Country) %>% summarise_all(n_distinct)
#Combinamos los dos dataframes
totalStats <- merge(sums,unic)
totalStats %>% formattable(align = c("c", "c", "c", "c", "c", "c"),
                           col.names = c("Volumen ventas","Beneficio obtenido", "Facturas emitidas","Nº de productos en Stock", "Clientes únicos", "Paises"))

```


#### Dia de la semana que más se vende

```{r weekday}
data %>%
  group_by(weekDay) %>%
  summarise(revenue = sum(lineTotal)) %>%
  ggplot(aes(x = weekDay, y = revenue)) + geom_col(fill='darkblue') + labs(x = 'Dia de la semana', y = 'Beneficio (£)', title = 'Beneficio por dia de la semana')
```

Se observa algo curioso, los jueves son el día que más se vende mientra que los domingos es el dia que menos.

#### Época del año en que más se vende
```{r monthly}
data %>%
  group_by(month) %>%
  summarise(revenue = sum(lineTotal)) %>%
  ggplot(aes(x = month, y = revenue)) + geom_col(fill='darkblue') + labs(x = 'Mes', y = 'Beneficio (£)', title = 'Beneficio por mes')
```

Observamos claramente que en el último cuatrimestre del año se vende mucho más que el resto.

#### Mapa de calor con los países

```{r map_wrangling}
#Mapa regiones vendidas
country <- data %>%
  group_by(Country) %>%
  summarise(SalesVolume = sum(Quantity)) %>%
  rename(region = Country)

#Como es un factor surge un problema al sustituir por tanto
#en este dataset lo convertiremos en string
country$region <- as.character(country$region)
country$region <- ifelse(country$region == "United Kingdom", "UK",country$region)
country$region <- ifelse(country$region == "EIRE", "Ireland",country$region)

#Como las ventas en UK son mucho mayores que en el resto de paises aplicamos logaritmos
country <- country %>%
  mutate(logVolume = log(SalesVolume),
         Proportion = (SalesVolume/sum(SalesVolume))*100)

country$Proportion <- round(country$Proportion,2)
```

Generamos el mapa de calor con los datos transformados a logaritmos, si no solo aparecería Reino Unido ya que vende muchisimo más:

```{r map}
world_map <- map_data("world")
salesmap <- full_join(country, world_map, by = "region")
ggplot(salesmap, aes(map_id = region, fill = logVolume))+
  geom_map(map = salesmap,  color = "white")+
  expand_limits(x = salesmap$long, y = salesmap$lat)+
  scale_fill_viridis_c(option = "C")
```

Creamos la tabla que representaba el gráfico anterior:

```{r country_table}
#Tabla ventas por paises y proporción
country %>%
  select(region,SalesVolume,Proportion) %>%
  arrange(desc(Proportion)) %>%
  formattable(align = c("l", "c", "c"),
              list(SalesVolume = color_bar("lightgrey"),
                   Proportion = 
                     formatter("span", style = ~ style(font.weight = "bold"))),
              col.names = c("País","Volumen Ventas", "Proporción"))
```

Se ha aplicado el logaritmo de las ventas porque como se puede observar en la tablas la mayoría de venta son en el mismo país, Reino Unido. Luego se vende en el resto de paises en menor medida pero se puede ver una clara diferencia una vez aplicados los logaritmos.

#### Regresión lineal

Comenzamos haciendo un regresión lineal simple:
```{r rl_simple}
rl_data <- data %>%
  select(lineTotal, Quantity, UnitPrice, Country)

fit1 <- lm(lineTotal ~ Quantity, data = rl_data)
summary(fit1)
```

```{r rl_plot, message='hide'}
#Representamos con logaritmos
plot(rl_data$Quantity,rl_data$lineTotal) +
abline(fit1)
```

Nos queda la siguiente regresión lineal simple:
$lineTotal = 1.95\beta_0+1.58Quantity$

Con un $R^2_a$ = 0.84, asi que el modelo ajusta muy bien.
Aparte de que todas las variables explicativas tienen un p-valor excelente.

Vamos a ampliar el modelo para observar si podemos linealizar el precio final de la cesta a partir de el precio y la cantidad del producto.
```{r rl_multiple}
fit2 <- lm(lineTotal ~. -Country, data = rl_data)
summary(fit2) #El coeficiente tiene el signo esperado (Positivo)
```

Por tanto nos queda un regresión lineal:

$lineTotal = -1.81\beta_0+1.58Quantity+1.20UnitPrice$

Con un $R^2_a$ = 0.84, asi que el modelo ajusta muy bien.
Aparte de que todas las variables explicativas tienen un p-valor excelente.

Un intervalo de confianza del 90% seria:
```{r confidence}
confint(fit2, level = 0.9)
```

Comprobamos si ampliar el modelo es realmente beneficioso haciendo un contraste de significación conjunto:
```{r anova}
anova(fit1,fit2)
```
Rechazamos hipótesis nula, por tanto, el modelo ampliado es mejor.

## Parte 3: Aprendizaje estadístico (Machine Learning) (40/100)

Algunos de los algoritmos aplicados en esta sección no tienen mucho sentido ya que los datos de los que disponemos estan bastante desbalanceados, por tanto, sus resultados pueden no ser correctos. En algunos casos se utilizaran submuestras y en otros se cogerán datos ligeramente modificados para que el análisis tenga algo de sentido. El objetivo de esta parte no es más que demostrar que se conocen y se saben aplicar este tipo de algoritmos. Finalmente en la última sección de esta parte se han añadido Reglas de Asociación así como una pequeña gráfica interactiva para poder observar mejor este tipo de patrones.

#### Algoritmos de clasificación

##### Boosting

Seleccionando variables como la cantidad comprada y el precio vamos a intentar predecir desde que país nos compran. Hemos cogido un submuestra para evitar esperas demasiado largas al ejectuar este algoritmo.

```{r boosting}
#Seleccionamos una submuestra para evitar tiempos muy largos de cálculo
df <- data[1:30000,c(4,6,8)]
df <- df[sample(nrow(df)),]  # shuffle

df.train <- df[1:15000,]
df.test <- df[15001:30000,]

BST = gbm(Country~.,data=df.train,
          distribution='multinomial',
          n.trees=200,
          interaction.depth=4,
          #cv.folds=5,
          shrinkage=0.005)

# La variable dependiente tiene varias categorias y usamos "multinomial".
predBST = predict(BST,n.trees=200, newdata=df.test,type='response')
pred=as.matrix(predBST[,,1])
# La funci�n which.max elige la categoria con la probabilidad m�s alta. 
p.predBST <- apply(predBST, 1, which.max)
# Añadimos nombres de categorias.
p.predcat <- colnames(predBST)[p.predBST]

table(p.predcat,df.test$Country)
mean(p.predcat==df.test$Country)
```
El boosting  ha clasificado bien el 91% de los paises en función de sus compras. Los datos estan desbalanceados, porque si el algoritmo elige como país Reino Unido, acertará la gran mayoría de veces porque es donde más venden en general.

##### Clasificación mediante logit

Vamos a clasificar si nos compra de Reino Unido o de un país tercero.

```{r logit}
#Como tenemos 37 paises vamos a cambiarlos por si es de Reino Unido o de fuera.
df$Country <- as.character(df$Country) #Para evitar problemas
df$Country <- ifelse(df$Country != "United Kingdom", "Fuera", df$Country)
df$Country <- as.factor(df$Country)

glm.fit=glm(Country~., data=df, family=binomial)
summary(glm.fit)

glm.probs=predict(glm.fit,type="response") 
glm.pred=ifelse(glm.probs>0.5,"United Kingdom","Fuera")

table(glm.pred,df$Country)
mean(glm.pred==df$Country)

```
Clasificamos con una precisión de 91% pero como se ha mencionado antes, esto no es fiable ya que los datos de este dataset estan muy desbalanceados por el hecho de que donde más vende es en Reino Unido con una diferencia abismal.

#### Análisis de componentes principales

Vamos a coger recoger la información del ecommerce por paises e intentar reducir su dimensión:

```{r pca}
paises <- data %>%
  select(Country, Quantity, UnitPrice, lineTotal) %>%
  group_by(Country) %>%
  summarise(Quantity = sum(Quantity),
            UnitPrice = sum(UnitPrice),
            lineTotal = sum(lineTotal))

pr.out <- prcomp(paises[,-1], scale = TRUE)
pr.out$rotation

#Seleccionamos el número de componentes
pr.out$sdev
#Con el criterio de la varianza, nos quedariamos con una sola componente
screeplot(pr.out, type = "l", main = "Varianzas de los Componentes Principales",
          col = "blue", cex.main = 0.8)
#En cambio con la regla del codo, cogeriamos 2 componentes principales
```

Nos quedamos con 2 componentes principales siguiendo la regla del codo y las representamos mediante un biplot:
```{r}
biplot(pr.out, scale=1)
```

#### Clustering por k-means

Vamos aplicar clustering para ver si podemos segmentar los datos en distintos grupos. Utilizamos los mismos datos que hemos utilizado en el apartado anterior. Pero eliminamos la observación referente a Reino Unido, si no Reino Unido estaria en un solo cluster y el resto de paises en otro.

```{r kmeans_cluster}
km_data <- paises[-35,2:3]

#Método del codo
set.seed(2021)
wcss = vector()
for(i in 1:10){
  wcss[i] <- sum(kmeans(km_data, i)$withinss)
}
plot(1:10, wcss, type = 'b', main= "Método del codo", xlab= "Número de clusters", ylab="WCSS")
```

Por tanto, el número óptimo de clusters para este algoritmo serian 2

```{r cluster_plot}
set.seed(2021)
kmeans <- kmeans(km_data, 2, iter.max = 300, nstart = 10)
clusplot(km_data,kmeans$cluster, lines = 0, shade = T, color = T, labels = 2, plotchar = F, span = T, main = "Clustering por paises", xlab= "Cantidad vendida", ylab = "Precio por unidad total")
```
Observamos que hay dos clusters de paises, el primer cluster incluye paises como: Australia, Irlanda, Francia, Alemania y Holanda. Básicamente paises vecinos (menos Australia) con los que tiene más volumen de ventas, es consistente con las tablas de volumen de ventas mostradas con anterioridad.

#### Reglas de asociación

Tenemos que tener los datos en un formato en concreto. Necesitamos los clientes en las filas y los productos adquiridos en columnas.
```{r arules, warning = FALSE}
a_data <- data %>%
  select(CustomerID, StockCode) %>%
  mutate(StockCode = strsplit(StockCode, ",")) %>% 
    unnest %>% 
    distinct(CustomerID, StockCode) %>% 
    group_by(CustomerID) %>% 
    summarise(StockCode = toString(StockCode))

```

Solo tendremos en cuenta cesta de compra de hasta 100 productos para evitar consumir demasiado espacio de memoria, ya que la mayoría de usuarios compra bastantes menos productos que estos.
```{r arules_2, results= 'hide', warning = FALSE}
a_data <- a_data %>% separate(StockCode, as.character(c(1:100)), ",")
#Convertimos en una matrix sparse para ahorrar más memoria
write.csv(a_data[,-1], "DatosTransacciones.csv")
dataset <- read.transactions("DatosTransacciones.csv", header = T, sep = ",", rm.duplicates = T)
```

Ahora que tenemos listos los datos, podemos comenzar a trabajar con las reglas de asociación.
```{r a_rules_sum}
summary(dataset)
```
De aquí podemos obtener un par de datos interesantes como que casi la totalidad de la matriz esta vacía, solo un 0.00005% esta ocupada por datos.

Los items (por su nombre de Stock) más vendidos son:
22423, 85123A, 47566, 84879, 22720, (Other), 801, 627, 609, 575, 199410

También podemos observar que 91 cestas de compra contenian sólo 2 items mientras que 780 cestas de compra contenian 101 items.

```{r a_rules_freq}
itemFrequencyPlot(dataset, topN = 25)
```

En el gráfico anterior se observan los 25 productos más vendidos.

Comenzamos creando las reglas de asociación:
```{r a_rules_train}
#Entrenamos el algoritmo Apriori
rules = apriori(dataset, list(support = 0.05, confidence = 0.8))
```
Hemos seleccionado un soporte de 0.05, por lo que no tratamos con items que son comprados rara vez, lo que significa que cortamos en el gráfico de frecuencias anterior por ese valor y nos han aparecido 5 reglas. Con el nivel de confianza de 0.8 significa que estas reglas se cumplen en 4 de cada 5 cestas de la compra.

```{r a_rules_sort}
inspect(sort(rules, by = "lift"))
```
Como tenemos los items nombrados por su código de stock esto no nos da mucha información, vamos a ver cuales son estos items:
```{r teacups}
data %>%
  select(StockCode, Description) %>%
  filter(StockCode == "22698" | StockCode == "22699"| StockCode == "22697") %>%
  unique()
```
Vemos que se trata de un set de tazas de té con su plato en colores diferentes.
Tiene sentido, ya que la mayoría de compras se hacen desde Reino Unido y allí beben mucho té.

```{r a_rules_exp}
#Volvemos a aplicar las reglas de asociación pero con una confianza un poco más baja
rules = apriori(dataset, list(support = 0.05, confidence = 0.6))
inspect(sort(rules, by = "lift")[1:10])
```
Vamos a ver que items nuevos aparecen ahora:
```{r teapots_paperchain}
data %>%
  select(StockCode, Description) %>%
  filter(StockCode == "22698" | StockCode == "22699"| StockCode == "22697" | StockCode == "22910" | StockCode == "22086") %>%
  unique()
```
Seguimos observando las combinaciones del set de tazas de té de diferentes colores pero ahora también han aparecido cadenas de papel con estilo navideño.
Si compran el set vintage, también compran el estilo de los años 50. Tiene sentido.

Aquí podemos observar las distintas reglas y como se aplican a los casos mencionados anteriormente de manera interactiva:
```{r a_rules_app}
reglas <- sort(rules, by = "lift")[1:10]
plot(reglas, method = "graph", engine = "htmlwidget")
```

## Apéndice

#### Librerias

Las librerias utilizadas en este analisis han sido:

**Tidyverse**: Es una libreria que contiene varias librerias muy útiles a la hora de manejar, transformar, seleccionar y representar datos. Se ha utilizado a lo largo de todo el trabajo.

**Lubridate**: Es una libreria que proporciona herramientas para facilitar el análisis de fechas y trabajar con datos temporales. Se ha utilizado en la sección *Limpieza de datos*.

**Formattable**: Es una libreria que proporciona herramientas para crear bonitas tablas en HTML. Se ha utilizado a lo largo de toda la parte de *Análisis Exploratorio (EDA)*.

**Arules**: Es una libreria que proporciona herramientas para poder analizar reglas de asociaciones y también contiene el algoritmo Apriori. Se ha utilizado en la sección *Reglas de Asociación*.

**ArulesViz**: Es una libreria que contiene herramientas para la visualización de las reglas de asociación del paquete anterior. Se ha utilizado al final de este documento para visualizar de manera interactivas las *Reglas de Asociación*.

**Ggplot2**: Es un subpaquete de *Tidyverse*. Se ha utilizado a lo largo de toda la parte de *Análisis Exploratorio (EDA)*.

**Cluster**: Es un paquete que contiene las herramientas básicas a la hora de analizar clusters. Se ha utilizado en la sección *Clustering por k-means*.

**Gbm**: Es un paquete que contiene herramientas para hacer regresiones con boosting.
Se ha utilizado en la sección de *Boosting*.

**ISLR**: Es un paquete que contiene herramientas básicas para hacer análisis estadístico.
Se ha utilizado en la sección de *Regresión Lineal*.
