# Generlizaed Random Forests (Beta)
install.packages("grf")
install.packages("devtools")
# Causal Learning (from github)

library(MASS)  # load the MASS package
library(randomForest)
library(grf)
library(Hmisc)

# This example is prepared for: 
#dataWN <- read.table(file="E:\\data\\Oregon\\oregon_puf\\OHIE_Public_Use_Files\\OHIE_QJE_Replication_Code\\WN_002NO1.txt", header=TRUE)
dataWN <- read.table(file="../../../dani_/Desktop/Aprendizaje Estadistico/DATA2018/WN_002NO1.txt", header=TRUE)
summary(dataWN)

# You can practice using: 
# dataWN <- read.table(file="E:\\data\\Oregon\\oregon_puf\\OHIE_Public_Use_Files\\OHIE_QJE_Replication_Code\\WN_002.txt", header=TRUE)
# summary(dataWN)
# Important: in this case the treatment variable is "Dohp_all_ever_admin2"
# This indicates if the individual ever was on Medicaid.

## set the seed to make your partition reproducible
set.seed(05112018)

## 70% of the sample size
smp_size <- floor(0.7 * nrow(dataWN))
train_ind <- sample(seq_len(nrow(dataWN)), size = smp_size)

train <- dataWN[train_ind, ]
test <- dataWN[-train_ind, ]

#Here I overwrite dataWN! (Maybe change this later), 
#Data includes "Dphqtot_inpD2", which is a dummy that can be used
#as the outcome instead of the quantitative variable that we use. 
#Below we drop the dummy variable, and we use the quantitative output.

dataWN <- subset( train, select = -c(Dphqtot_inpD2) )
dataWNt <- subset( test, select = -c(Dphqtot_inpD2) )

#Separate into Y, X, Z for Causal Forest
#Winning the lottery is indicated "by Dtreatment2", which is our instrumental variable
Y <- subset( dataWN, select = c(phqtot_inp) )
Z <- subset( dataWN, select = c(Dtreatment2) )
W <- subset( dataWN, select = c(Dins_noins_12m2) )
X <- subset( dataWN, select = -c(phqtot_inp) )
X <- subset( X, select = -c(Dtreatment2) )
X <- subset( X, select = -c(Dins_noins_12m2) )

w1 <-as.matrix(W)
x1 <-as.matrix(X)
y1 <-as.matrix(Y)
z1 <-as.matrix(Z)

#Test
Xt <- subset( dataWNt, select = -c(phqtot_inp) )
Xt <- subset( Xt, select = -c(Dtreatment2) )
Xt <- subset( Xt, select = -c(Dins_noins_12m2) )

xt1 <-as.matrix(Xt)

# Use train. The model uses local centering.
iforest<-instrumental_forest(x1, y1, w1, z1)
iforest
variable_importance(iforest)

# Use train. In this case we make the local centering in advance.
# regression_forest is a funcion in grf-package.
forest.W = regression_forest(x1, w1, tune.parameters = "all")
W.hat = predict(forest.W)$predictions

forest.Y = regression_forest(x1, y1, tune.parameters = "all")
forest.Y.varimp = variable_importance(forest.Y)
Y.hat = predict(forest.Y)$predictions

forest.Z = regression_forest(x1, z1, tune.parameters = "all")
Z.hat = predict(forest.Z)$predictions

tau.iv.forest = instrumental_forest (x1, y1, w1, z1,
                                     W.hat = W.hat, Y.hat = Y.hat, Z.hat= Z.hat
)

#Order and include names
importance_df <- data.frame(variable_importance(tau.iv.forest),  rownames = colnames(X))
importance_df <-format(importance_df, scientific=FALSE)
colnames(importance_df) <- c("importance", "variable")
importance_df1<-importance_df[order(importance_df$importance, decreasing=TRUE),]
importance_df1

# Create figures. 
tau.iv.forest.ci = instrumental_forest (x1, y1, w1, z1,
                                        W.hat = W.hat, Y.hat = Y.hat, Z.hat= Z.hat,
                                        num.trees = 4000
)

forest.Y.varimp = variable_importance(tau.iv.forest.ci)

# Note: Forests may have a hard time when trained on very few variables
# (e.g., ncol(X) = 1, 2, or 3). We recommend not being too aggressive
# in selection.
selected.vars = which(forest.Y.varimp / mean(forest.Y.varimp) > 0.2)

tau.iv.forest.ci = instrumental_forest (x1[, selected.vars], y1, w1, z1,
                                        W.hat = W.hat, Y.hat = Y.hat, Z.hat= Z.hat,
                                        num.trees = 4000
)

importance_df.ci <- data.frame(variable_importance(tau.iv.forest.ci),  rownames = colnames(x1[, selected.vars]))
importance_df.ci <-format(importance_df, scientific=FALSE)
colnames(importance_df.ci) <- c("importance", "variable")
importance_df1.ci<-importance_df.ci[order(importance_df.ci$importance, decreasing=TRUE),]
importance_df1.ci

#START MAKING GRAPH
#X.test = matrix(1, nrow(x2h), ncol(x2h[, selected.vars]))
X.test = (x1[, selected.vars])
X.test2 <- X.test

summary(X.test2)

# RUN the code until here. Check variable importance, 
# and see the summary to find the number of the variable.
# Choose variable number to make graph for. Remember to change the label on the X-axis.
# If you want to make another graph, remember to run the code from 
# the start of "MAKING GRAPH".
varnum <- 1

#I prepare the graph so that only p5 to p95 is included, but for some variables
#this could be incovenient. We can change it manually in seq(...)
d5d95 <- quantile(X.test2[,varnum], prob=c(0.05,0.95))
d5d95m <- as.matrix(d5d95)
d5 <- d5d95m[1,1]
d95 <- d5d95m[2,1]

p5 <-round(d5, digits = 1)
p5 <- unname(p5)

p95 <-round(d95, digits = 1)
p95 <- unname(p95)

#All other variables are set to their median
for(i in 1:ncol(X.test2)){
  X.test2[,i] <- median(X.test2[,i], na.rm = TRUE)
}
#A sequence for a relevant range of the variable is specified.
X.test2[,varnum] = seq(p5, p95, length.out = nrow(X.test2))

tau.hat = predict(tau.iv.forest.ci, X.test2, estimate.variance = TRUE)
sigma.hat = sqrt(tau.hat$variance.estimates)

ylim = range(tau.hat$predictions + 1.96 * sigma.hat, tau.hat$predictions - 1.96 * sigma.hat, 0, 2)
plot(X.test2[,varnum], tau.hat$predictions, ylim = ylim, xlab = "birthyear", ylab = "tau", type = "l")
lines(X.test2[,varnum], tau.hat$predictions + 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test2[,varnum], tau.hat$predictions - 1.96 * sigma.hat, col = 1, lty = 2)
# UNTIL HERE

#Predict on new data
pred_ivf = predict(tau.iv.forest, newdata = Xt)
pred_ivf1 = pred_ivf$predictions 
summary(pred_ivf1)

#We have some really extreme outliers..
boxplot(pred_ivf1)
#...and we can show the boxplot without them:
boxplot(pred_ivf1, outline = FALSE)

#As a general rule we should be careful to remove outliers.
remove_outliers <- function(x, na.rm = TRUE, ...) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm, ...)
  H <- 1.5 * IQR(x, na.rm = na.rm)
  y <- x
  y[x < (qnt[1] - H)] <- NA
  y[x > (qnt[2] + H)] <- NA
  y
}

pred_ivf1drop <- remove_outliers(pred_ivf1)
hist(pred_ivf1drop)

# Estimate propensity score to check common support
# We cannot use the outcome to explain the treatment, so we drop it:
Xrf = subset( dataWN, select = -c(phqtot_inp) )
# We use random forest below, and the treatment is a qualitative variable.
Xrf$Dins_noins_12m2 = as.factor(Xrf$Dins_noins_12m2)
Xrf = subset( Xrf, select = -c(Dtreatment2) )

# Random Forests. In practice you should tune "mtry", but here we 
# just illustrate how it works.
rf.medicaid=randomForest(Dins_noins_12m2~.,data=Xrf,mtry=5)
importance(rf.medicaid)
pscore = predict(rf.medicaid, Xrf, type = "prob")
pscore1 = as.matrix(pscore[,2])

# Before using our strata we should check for common support. 
support<-cbind(pscore1,Xrf$Dins_noins_12m2)
supportdf <-as.data.frame(support)
supportdf$V2<-as.factor(supportdf$V2)

#Plot the density of the probability for both group to see common support. 
p<-ggplot(supportdf, aes(x=V1, fill=V2)) +
  geom_density(alpha=0.4)
p



